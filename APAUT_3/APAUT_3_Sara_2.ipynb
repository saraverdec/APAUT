{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a61b1c",
   "metadata": {},
   "source": [
    "Librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe7a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48cfc4",
   "metadata": {},
   "source": [
    "Cargamos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ee5d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100_g_at</th>\n",
       "      <th>1000_at</th>\n",
       "      <th>1001_at</th>\n",
       "      <th>1002_f_at</th>\n",
       "      <th>1003_s_at</th>\n",
       "      <th>1004_at</th>\n",
       "      <th>1005_at</th>\n",
       "      <th>1006_at</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1008_f_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "      <th>AFFX-YEL002c/WBP1_at</th>\n",
       "      <th>AFFX-YEL018w/_at</th>\n",
       "      <th>AFFX-YEL021w/URA3_at</th>\n",
       "      <th>AFFX-YEL024w/RIP1_at</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.927460</td>\n",
       "      <td>7.391657</td>\n",
       "      <td>3.812922</td>\n",
       "      <td>3.453385</td>\n",
       "      <td>6.070151</td>\n",
       "      <td>5.527153</td>\n",
       "      <td>5.812353</td>\n",
       "      <td>3.167275</td>\n",
       "      <td>7.354981</td>\n",
       "      <td>9.419909</td>\n",
       "      <td>...</td>\n",
       "      <td>3.770583</td>\n",
       "      <td>2.884436</td>\n",
       "      <td>2.730025</td>\n",
       "      <td>3.126168</td>\n",
       "      <td>2.870161</td>\n",
       "      <td>3.082210</td>\n",
       "      <td>2.747289</td>\n",
       "      <td>3.226588</td>\n",
       "      <td>3.480196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.222432</td>\n",
       "      <td>7.329050</td>\n",
       "      <td>3.958028</td>\n",
       "      <td>3.407226</td>\n",
       "      <td>5.921265</td>\n",
       "      <td>5.376464</td>\n",
       "      <td>7.303408</td>\n",
       "      <td>3.108708</td>\n",
       "      <td>7.391872</td>\n",
       "      <td>10.539579</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190759</td>\n",
       "      <td>2.460119</td>\n",
       "      <td>2.696578</td>\n",
       "      <td>2.675271</td>\n",
       "      <td>2.940032</td>\n",
       "      <td>3.126269</td>\n",
       "      <td>3.013745</td>\n",
       "      <td>3.517859</td>\n",
       "      <td>3.428752</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.776402</td>\n",
       "      <td>7.664007</td>\n",
       "      <td>3.783702</td>\n",
       "      <td>3.152019</td>\n",
       "      <td>5.452293</td>\n",
       "      <td>5.111794</td>\n",
       "      <td>7.207638</td>\n",
       "      <td>3.077360</td>\n",
       "      <td>7.488371</td>\n",
       "      <td>6.833428</td>\n",
       "      <td>...</td>\n",
       "      <td>3.325183</td>\n",
       "      <td>2.603014</td>\n",
       "      <td>2.469759</td>\n",
       "      <td>2.615746</td>\n",
       "      <td>2.510172</td>\n",
       "      <td>2.730814</td>\n",
       "      <td>2.613696</td>\n",
       "      <td>2.823436</td>\n",
       "      <td>3.049716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.919134</td>\n",
       "      <td>7.469634</td>\n",
       "      <td>4.004581</td>\n",
       "      <td>3.341170</td>\n",
       "      <td>6.070925</td>\n",
       "      <td>5.296108</td>\n",
       "      <td>8.744059</td>\n",
       "      <td>3.117104</td>\n",
       "      <td>7.203028</td>\n",
       "      <td>10.400557</td>\n",
       "      <td>...</td>\n",
       "      <td>3.625057</td>\n",
       "      <td>2.765521</td>\n",
       "      <td>2.681757</td>\n",
       "      <td>3.310741</td>\n",
       "      <td>3.197177</td>\n",
       "      <td>3.414182</td>\n",
       "      <td>3.193867</td>\n",
       "      <td>3.353537</td>\n",
       "      <td>3.567482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.113561</td>\n",
       "      <td>7.322408</td>\n",
       "      <td>4.242724</td>\n",
       "      <td>3.489324</td>\n",
       "      <td>6.141657</td>\n",
       "      <td>5.628390</td>\n",
       "      <td>6.825370</td>\n",
       "      <td>3.794904</td>\n",
       "      <td>7.403024</td>\n",
       "      <td>10.240322</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698067</td>\n",
       "      <td>3.026876</td>\n",
       "      <td>2.691670</td>\n",
       "      <td>3.236030</td>\n",
       "      <td>3.003906</td>\n",
       "      <td>3.081497</td>\n",
       "      <td>2.963307</td>\n",
       "      <td>3.472050</td>\n",
       "      <td>3.598103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   100_g_at   1000_at   1001_at  1002_f_at  1003_s_at   1004_at   1005_at  \\\n",
       "0  6.927460  7.391657  3.812922   3.453385   6.070151  5.527153  5.812353   \n",
       "1  7.222432  7.329050  3.958028   3.407226   5.921265  5.376464  7.303408   \n",
       "2  6.776402  7.664007  3.783702   3.152019   5.452293  5.111794  7.207638   \n",
       "3  6.919134  7.469634  4.004581   3.341170   6.070925  5.296108  8.744059   \n",
       "4  7.113561  7.322408  4.242724   3.489324   6.141657  5.628390  6.825370   \n",
       "\n",
       "    1006_at  1007_s_at  1008_f_at  ...  AFFX-ThrX-5_at  AFFX-ThrX-M_at  \\\n",
       "0  3.167275   7.354981   9.419909  ...        3.770583        2.884436   \n",
       "1  3.108708   7.391872  10.539579  ...        3.190759        2.460119   \n",
       "2  3.077360   7.488371   6.833428  ...        3.325183        2.603014   \n",
       "3  3.117104   7.203028  10.400557  ...        3.625057        2.765521   \n",
       "4  3.794904   7.403024  10.240322  ...        3.698067        3.026876   \n",
       "\n",
       "   AFFX-TrpnX-3_at  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  AFFX-YEL002c/WBP1_at  \\\n",
       "0         2.730025         3.126168         2.870161              3.082210   \n",
       "1         2.696578         2.675271         2.940032              3.126269   \n",
       "2         2.469759         2.615746         2.510172              2.730814   \n",
       "3         2.681757         3.310741         3.197177              3.414182   \n",
       "4         2.691670         3.236030         3.003906              3.081497   \n",
       "\n",
       "   AFFX-YEL018w/_at  AFFX-YEL021w/URA3_at  AFFX-YEL024w/RIP1_at  Y  \n",
       "0          2.747289              3.226588              3.480196  0  \n",
       "1          3.013745              3.517859              3.428752  1  \n",
       "2          2.613696              2.823436              3.049716  0  \n",
       "3          3.193867              3.353537              3.567482  0  \n",
       "4          2.963307              3.472050              3.598103  1  \n",
       "\n",
       "[5 rows x 12626 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('prostate.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e913af",
   "metadata": {},
   "source": [
    "Y extraemos las etiquetas, que están en la última columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23750923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1\n",
      " 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.iloc[:, :-1]).astype(float)\n",
    "y = np.array(data.iloc[:, -1]).astype(int)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5294ad",
   "metadata": {},
   "source": [
    "**You are asked to:**\n",
    "\n",
    "**1. Estimate the performance of the nearest neighbor classifier on this dataset using 10-times 10-fold cross validation when all the features are used for prediction. The number of neighbors should be chosen using an inner cross-validation procedure. You can use 5-fold cross validation for this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba89c8",
   "metadata": {},
   "source": [
    "En primer lugar, tenemos que dividir el conjunto de datos en 10 partes (10-fold). De ellas, 9 se utilizarán para el entrenamiento del clasificador y una para el test. Este proceso se repite 10 veces (10-times).\n",
    "\n",
    "Por otro lado, antes de entrenar el clasificador Nearest Neighbors, tenemos que averiguar qué número de vecinos (k) minimiza el error de predicción. Para ello, volvemos a dividir los datos en 5 partes. 4 de ellas se utilizarán para entrenar el clasificador K-Nearest-Neighbors con cada número de vecinos, y una para testarlo. Este proceso se realizará con cada uno de los conjuntos de entrenamiento generados en el paso anterior, es decir, 10 veces.\n",
    "\n",
    "Una vez seleccionado el mejor modelo, se utilizará para realizar las predicciones, que se compararán con las etiquetas reales del conjunto de test para calcular la precisión en cada caso.\n",
    "\n",
    "La precisión global del clasificador será el promedio de las accuracies obtenidas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb527b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">La precisión también podría calcularse como el número de aciertos entre el número total de predicciones, como hace él en los notebooks. No sé si lo ha hecho de esta forma alguna vez</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7883cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy for this fold: 0.9090909090909091\n",
      "Fold 2\n",
      "Accuracy for this fold: 0.8181818181818182\n",
      "Fold 3\n",
      "Accuracy for this fold: 0.8\n",
      "Fold 4\n",
      "Accuracy for this fold: 0.7\n",
      "Fold 5\n",
      "Accuracy for this fold: 0.8\n",
      "Fold 6\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 7\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 8\n",
      "Accuracy for this fold: 0.8\n",
      "Fold 9\n",
      "Accuracy for this fold: 0.8\n",
      "Fold 10\n",
      "Accuracy for this fold: 0.6\n",
      "Mean accuracy across all folds: 0.8127272727272727\n",
      "Standard deviation of accuracy: 0.10517203537439689\n"
     ]
    }
   ],
   "source": [
    "# Definimos el clasificador KNN\n",
    "knn = KNeighborsClassifier()\n",
    "# Definimos el rango de valores que puede tomar n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "# Establecemos en qué nos vamos a basar para evaluar el rendimiento del modelo\n",
    "scoring = make_scorer(accuracy_score)\n",
    "# Creamos una lista para almacenar las precisiones de cada pliegue\n",
    "accuracies = []\n",
    "# Establecemos las condiciones para la validación cruzada externa\n",
    "out_cv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "fold = 1\n",
    "\n",
    "for train_idx, test_idx in out_cv.split(X, y):\n",
    "\n",
    "    print(f\"Fold {fold}\")\n",
    "    # Generamos los conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    # Normalizamos los datos\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # Buscamos el mejor valor de n_neighbors para el clasificador KNN\n",
    "    in_cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1, random_state = 0)\n",
    "    gridcv = GridSearchCV(estimator = knn, param_grid = param_grid, scoring = scoring, cv = in_cv, n_jobs=1)\n",
    "    gridcv.fit(X_train_scaled, y_train)\n",
    "    # Obtenemos el mejor modelo\n",
    "    best_knn = gridcv.best_estimator_\n",
    "    # Utilizamos el mejor modelo para hacer predicciones sobre el conjunto de prueba\n",
    "    predictions = best_knn.predict(X_test_scaled)\n",
    "    # Calculamos la precisión del modelo\n",
    "    test_accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy for this fold: {test_accuracy}\")\n",
    "    # Almacenamos el valor de precisión para poder calcular la media y la desviación estándar\n",
    "    accuracies.append(test_accuracy)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "print(f\"Mean accuracy across all folds: {mean_accuracy}\")\n",
    "print(f\"Standard deviation of accuracy: {std_accuracy}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d09be8",
   "metadata": {},
   "source": [
    "**2. Estimate the performance of the nearest neighbor classifier on the same dataset when using a feature selection technique based on the F-score (ANOVA) that picks up the 10 most relevant features. Use the same cross-validation methods as in the previous step.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd2a65",
   "metadata": {},
   "source": [
    "Al código anterior le vamos a agregar un paso de selección de atributos basado en el F-score (ANOVA), que es una comparación de la varianza de cada característica entre clases con respecto a la varianza de dicha característica dentro de cada clase. Cuanto mayor es el F-score, más relevante es una característica a la hora de clasificar los datos.\n",
    "\n",
    "Por tanto, antes de entrenar el clasificador KNN seleccionaremos los 10 atributos con mayor F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ac7573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 2\n",
      "Accuracy for this fold: 0.9090909090909091\n",
      "Fold 3\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 4\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 5\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 6\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 7\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 8\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 9\n",
      "Accuracy for this fold: 0.8\n",
      "Fold 10\n",
      "Accuracy for this fold: 0.7\n",
      "Mean accuracy across all folds: 0.8909090909090909\n",
      "Standard deviation of accuracy: 0.08322030902796951\n"
     ]
    }
   ],
   "source": [
    "# Definimos el clasificador KNN\n",
    "knn = KNeighborsClassifier()\n",
    "# Definimos el rango de valores que puede tomar n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "# Establecemos en qué nos vamos a basar para evaluar el rendimiento del modelo\n",
    "scoring = make_scorer(accuracy_score)\n",
    "# Creamos una lista para almacenar las precisiones de cada pliegue\n",
    "accuracies = []\n",
    "# Establecemos las condiciones para la validación cruzada externa\n",
    "out_cv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "fold = 1\n",
    "\n",
    "for train_idx, test_idx in out_cv.split(X, y):\n",
    "\n",
    "    print(f\"Fold {fold}\")\n",
    "    # Generamos los conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    # Normalizamos los datos\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # Seleccionamos los 10 atributos más relevantes según el F-score\n",
    "    feature_selector = SelectKBest(f_classif, k=10)\n",
    "    X_train_selection = feature_selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_selection = feature_selector.transform(X_test_scaled)\n",
    "    # Buscamos el mejor valor de n_neighbors para el clasificador KNN\n",
    "    in_cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1, random_state = 0)\n",
    "    gridcv = GridSearchCV(estimator = knn, param_grid = param_grid, scoring = scoring, cv = in_cv, n_jobs=1)\n",
    "    gridcv.fit(X_train_selection, y_train)\n",
    "    # Obtenemos el mejor modelo\n",
    "    best_knn = gridcv.best_estimator_\n",
    "    # Utilizamos el mejor modelo para hacer predicciones sobre el conjunto de prueba\n",
    "    predictions = best_knn.predict(X_test_selection)\n",
    "    # Calculamos la precisión del modelo\n",
    "    test_accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy for this fold: {test_accuracy}\")\n",
    "    # Almacenamos el valor de precisión para poder calcular la media y la desviación estándar\n",
    "    accuracies.append(test_accuracy)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "print(f\"Mean accuracy across all folds: {mean_accuracy}\")\n",
    "print(f\"Standard deviation of accuracy: {std_accuracy}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1dfdc0",
   "metadata": {},
   "source": [
    "**3. Repeat the previous experiment but when a random forest is used to pick up the 10 most relevant features. Use an initial filtering method based on the F-score to keep only the 20% most promising features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87989d4",
   "metadata": {},
   "source": [
    "Ahora utilizaremos el F-score para quedarnos con un 20% de los atributos totales, que serán aquellos que parecen ser más relevantes. De ese 20%, las 10 mejores características serán seleccionadas utilizando el método random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b197251",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">He puesto n_estimators=2000 como Leo, pero se podría modificar el código para que escoja el valor de n_estimators óptimo. De hecho, creo que habría que hacerlo así :/</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da6fbe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 2\n",
      "Accuracy for this fold: 0.7272727272727273\n",
      "Fold 3\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 4\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 5\n",
      "Accuracy for this fold: 0.8\n",
      "Fold 6\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 7\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 8\n",
      "Accuracy for this fold: 1.0\n",
      "Fold 9\n",
      "Accuracy for this fold: 0.9\n",
      "Fold 10\n",
      "Accuracy for this fold: 0.7\n",
      "Mean accuracy across all folds: 0.9027272727272726\n",
      "Standard deviation of accuracy: 0.1139123920293628\n"
     ]
    }
   ],
   "source": [
    "# Definimos el clasificador KNN\n",
    "knn = KNeighborsClassifier()\n",
    "# Definimos el rango de valores que puede tomar n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "# Establecemos en qué nos vamos a basar para evaluar el rendimiento del modelo\n",
    "scoring = make_scorer(accuracy_score)\n",
    "# Creamos una lista para almacenar las precisiones de cada pliegue\n",
    "accuracies = []\n",
    "# Establecemos las condiciones para la validación cruzada externa\n",
    "out_cv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "# Definimos el método Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=2000, random_state=1)\n",
    "\n",
    "fold = 1\n",
    "\n",
    "for train_idx, test_idx in out_cv.split(X, y):\n",
    "\n",
    "    print(f\"Fold {fold}\")\n",
    "    # Generamos los conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    # Normalizamos los datos\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # Seleccionamos los atributos más relevantes según el F-score (20%)\n",
    "    feature_selector_fscore = SelectKBest(f_classif, k=int(0.2*X_train.shape[1]))\n",
    "    X_train_sel_fscore = feature_selector_fscore.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_sel_fscore = feature_selector_fscore.transform(X_test_scaled)\n",
    "    # Seleccionamos los 10 atributos más relevantes según random forest\n",
    "    rf.fit(X_train_sel_fscore, y_train)\n",
    "    feature_selector_rf = SelectFromModel(rf, max_features=10)\n",
    "    X_train_sel_rf = feature_selector_rf.transform(X_train_sel_fscore)\n",
    "    X_test_sel_rf = feature_selector_rf.transform(X_test_sel_fscore)\n",
    "    # Buscamos el mejor valor de n_neighbors para el clasificador KNN\n",
    "    in_cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1, random_state = 0)\n",
    "    gridcv = GridSearchCV(estimator = knn, param_grid = param_grid, scoring = scoring, cv = in_cv, n_jobs=1)\n",
    "    gridcv.fit(X_train_sel_rf, y_train)\n",
    "    # Obtenemos el mejor modelo\n",
    "    best_knn = gridcv.best_estimator_\n",
    "    # Utilizamos el mejor modelo para hacer predicciones sobre el conjunto de prueba\n",
    "    predictions = best_knn.predict(X_test_sel_rf)\n",
    "    # Calculamos la precisión del modelo\n",
    "    test_accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy for this fold: {test_accuracy}\")\n",
    "    # Almacenamos el valor de precisión para poder calcular la media y la desviación estándar\n",
    "    accuracies.append(test_accuracy)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "print(f\"Mean accuracy across all folds: {mean_accuracy}\")\n",
    "print(f\"Standard deviation of accuracy: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b12c29",
   "metadata": {},
   "source": [
    "**4. What feature selection method performs best? Can you explain why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b2ba0",
   "metadata": {},
   "source": [
    "En ambos casos la precisión aumenta con respecto a la situación inicial, en la que no se seleccionaban las características más relevantes. Esto puede deberse a que en el conjunto de atributos original hay atributos irrelevantes o redundantes que introducen ruido.\n",
    "\n",
    "Sin embargo, al seleccionar los atributos más relevantes con random forest obtenemos mayor precisión que al seleccionarlos según el F-score.\n",
    "\n",
    "La selección en función del F-score actúa a modo de filtro, antes del entrenamiento del clasificador. No tiene en cuenta la naturaleza de éste y, además, considera las características de forma independiente, por lo que es posible que características que solo son relevantes si van juntas pasen desapercibidas.\n",
    "\n",
    "Por otra parte, con random forest sí es posible identificar interacciones entre características, por lo que es posible que atributos que antes no se estaban teniendo en cuenta sí se estén considerando en este caso y eso es lo que hace que la precisión aumente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
