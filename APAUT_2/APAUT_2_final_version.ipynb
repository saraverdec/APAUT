{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Classification Systems\n",
    "Por Enrique Juliá Arévalo, Sara Verde Camacho y Leo Pérez Peña"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta práctica es comparar el error de predicción de los siguientes clasificadores:\n",
    "- Naive Bayes Classifier\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Quadratic Discriminant Analysis (QDA)\n",
    "- Nearest Shrunken Centroids Classifier\n",
    "\n",
    "Utilizando el conjunto de datos de cáncer de mama wdbc.csv y el conjunto de datos de cáncer de próstata prostate.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librerías necesarias\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, cargamos los datos y extraemos las dimensiones de las tablas para hacernos una idea de cuántos datos tenemos en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_data = pd.read_csv('wdbc.csv', header=None)\n",
    "breast_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 12626)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prostate_data = pd. read_csv('prostate.csv')\n",
    "prostate_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el primer caso, el número de datos es mucho mayor que el número de dimensiones. Sin embargo, en el segundo caso, el número de dimensiones es mucho mayor que el número de datos. Esto probablemente afectará al rendimiento de los distintos clasificadores, ya que unos clasificadores son más adecuados en un caso y otros en otro.\n",
    "\n",
    "Una vez cargados los datos, extraemos las etiquetas.\n",
    "\n",
    "En el primer conjunto de datos, las etiquetas están en la segunda columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1      2      3       4       5        6        7        8   \\\n",
       "0      842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
       "564    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
       "565    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
       "566    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
       "567    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
       "568     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
       "\n",
       "          9   ...      22     23      24      25       26       27      28  \\\n",
       "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X_breast = breast_data.values[:, 2:].astype(float)\n",
    "y_breast = (breast_data.values[:, 1] == 'M').astype(int) # 1 para maligno, 0 para benigno\n",
    "print(y_breast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el segundo conjunto de datos, las estiquetas están en la última columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100_g_at</th>\n",
       "      <th>1000_at</th>\n",
       "      <th>1001_at</th>\n",
       "      <th>1002_f_at</th>\n",
       "      <th>1003_s_at</th>\n",
       "      <th>1004_at</th>\n",
       "      <th>1005_at</th>\n",
       "      <th>1006_at</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1008_f_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "      <th>AFFX-YEL002c/WBP1_at</th>\n",
       "      <th>AFFX-YEL018w/_at</th>\n",
       "      <th>AFFX-YEL021w/URA3_at</th>\n",
       "      <th>AFFX-YEL024w/RIP1_at</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.927460</td>\n",
       "      <td>7.391657</td>\n",
       "      <td>3.812922</td>\n",
       "      <td>3.453385</td>\n",
       "      <td>6.070151</td>\n",
       "      <td>5.527153</td>\n",
       "      <td>5.812353</td>\n",
       "      <td>3.167275</td>\n",
       "      <td>7.354981</td>\n",
       "      <td>9.419909</td>\n",
       "      <td>...</td>\n",
       "      <td>3.770583</td>\n",
       "      <td>2.884436</td>\n",
       "      <td>2.730025</td>\n",
       "      <td>3.126168</td>\n",
       "      <td>2.870161</td>\n",
       "      <td>3.082210</td>\n",
       "      <td>2.747289</td>\n",
       "      <td>3.226588</td>\n",
       "      <td>3.480196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.222432</td>\n",
       "      <td>7.329050</td>\n",
       "      <td>3.958028</td>\n",
       "      <td>3.407226</td>\n",
       "      <td>5.921265</td>\n",
       "      <td>5.376464</td>\n",
       "      <td>7.303408</td>\n",
       "      <td>3.108708</td>\n",
       "      <td>7.391872</td>\n",
       "      <td>10.539579</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190759</td>\n",
       "      <td>2.460119</td>\n",
       "      <td>2.696578</td>\n",
       "      <td>2.675271</td>\n",
       "      <td>2.940032</td>\n",
       "      <td>3.126269</td>\n",
       "      <td>3.013745</td>\n",
       "      <td>3.517859</td>\n",
       "      <td>3.428752</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.776402</td>\n",
       "      <td>7.664007</td>\n",
       "      <td>3.783702</td>\n",
       "      <td>3.152019</td>\n",
       "      <td>5.452293</td>\n",
       "      <td>5.111794</td>\n",
       "      <td>7.207638</td>\n",
       "      <td>3.077360</td>\n",
       "      <td>7.488371</td>\n",
       "      <td>6.833428</td>\n",
       "      <td>...</td>\n",
       "      <td>3.325183</td>\n",
       "      <td>2.603014</td>\n",
       "      <td>2.469759</td>\n",
       "      <td>2.615746</td>\n",
       "      <td>2.510172</td>\n",
       "      <td>2.730814</td>\n",
       "      <td>2.613696</td>\n",
       "      <td>2.823436</td>\n",
       "      <td>3.049716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.919134</td>\n",
       "      <td>7.469634</td>\n",
       "      <td>4.004581</td>\n",
       "      <td>3.341170</td>\n",
       "      <td>6.070925</td>\n",
       "      <td>5.296108</td>\n",
       "      <td>8.744059</td>\n",
       "      <td>3.117104</td>\n",
       "      <td>7.203028</td>\n",
       "      <td>10.400557</td>\n",
       "      <td>...</td>\n",
       "      <td>3.625057</td>\n",
       "      <td>2.765521</td>\n",
       "      <td>2.681757</td>\n",
       "      <td>3.310741</td>\n",
       "      <td>3.197177</td>\n",
       "      <td>3.414182</td>\n",
       "      <td>3.193867</td>\n",
       "      <td>3.353537</td>\n",
       "      <td>3.567482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.113561</td>\n",
       "      <td>7.322408</td>\n",
       "      <td>4.242724</td>\n",
       "      <td>3.489324</td>\n",
       "      <td>6.141657</td>\n",
       "      <td>5.628390</td>\n",
       "      <td>6.825370</td>\n",
       "      <td>3.794904</td>\n",
       "      <td>7.403024</td>\n",
       "      <td>10.240322</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698067</td>\n",
       "      <td>3.026876</td>\n",
       "      <td>2.691670</td>\n",
       "      <td>3.236030</td>\n",
       "      <td>3.003906</td>\n",
       "      <td>3.081497</td>\n",
       "      <td>2.963307</td>\n",
       "      <td>3.472050</td>\n",
       "      <td>3.598103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.819913</td>\n",
       "      <td>7.394450</td>\n",
       "      <td>3.617486</td>\n",
       "      <td>3.032897</td>\n",
       "      <td>5.414759</td>\n",
       "      <td>5.254673</td>\n",
       "      <td>8.785910</td>\n",
       "      <td>3.671019</td>\n",
       "      <td>7.670415</td>\n",
       "      <td>9.357480</td>\n",
       "      <td>...</td>\n",
       "      <td>3.922944</td>\n",
       "      <td>2.782813</td>\n",
       "      <td>2.520608</td>\n",
       "      <td>2.885149</td>\n",
       "      <td>2.783539</td>\n",
       "      <td>2.554428</td>\n",
       "      <td>2.733469</td>\n",
       "      <td>3.068130</td>\n",
       "      <td>2.953872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>7.106082</td>\n",
       "      <td>7.465860</td>\n",
       "      <td>4.091138</td>\n",
       "      <td>3.402607</td>\n",
       "      <td>5.894284</td>\n",
       "      <td>5.206407</td>\n",
       "      <td>8.796626</td>\n",
       "      <td>3.154433</td>\n",
       "      <td>7.724618</td>\n",
       "      <td>7.752833</td>\n",
       "      <td>...</td>\n",
       "      <td>3.491258</td>\n",
       "      <td>2.830696</td>\n",
       "      <td>2.688828</td>\n",
       "      <td>3.054819</td>\n",
       "      <td>2.859602</td>\n",
       "      <td>2.883547</td>\n",
       "      <td>2.651160</td>\n",
       "      <td>3.306874</td>\n",
       "      <td>3.234820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>6.907847</td>\n",
       "      <td>7.076667</td>\n",
       "      <td>3.998270</td>\n",
       "      <td>3.204424</td>\n",
       "      <td>5.912380</td>\n",
       "      <td>5.486270</td>\n",
       "      <td>6.133782</td>\n",
       "      <td>3.874307</td>\n",
       "      <td>7.153055</td>\n",
       "      <td>10.432679</td>\n",
       "      <td>...</td>\n",
       "      <td>3.512675</td>\n",
       "      <td>2.590794</td>\n",
       "      <td>2.384583</td>\n",
       "      <td>2.890110</td>\n",
       "      <td>2.785744</td>\n",
       "      <td>2.755482</td>\n",
       "      <td>2.731221</td>\n",
       "      <td>3.617421</td>\n",
       "      <td>3.255625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.767213</td>\n",
       "      <td>7.170628</td>\n",
       "      <td>3.520863</td>\n",
       "      <td>3.413946</td>\n",
       "      <td>6.294376</td>\n",
       "      <td>5.528486</td>\n",
       "      <td>5.160356</td>\n",
       "      <td>3.367304</td>\n",
       "      <td>7.022459</td>\n",
       "      <td>11.443175</td>\n",
       "      <td>...</td>\n",
       "      <td>3.654453</td>\n",
       "      <td>3.125644</td>\n",
       "      <td>2.593132</td>\n",
       "      <td>3.655418</td>\n",
       "      <td>2.668599</td>\n",
       "      <td>2.901230</td>\n",
       "      <td>2.651258</td>\n",
       "      <td>3.174388</td>\n",
       "      <td>3.263090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>7.066264</td>\n",
       "      <td>7.044501</td>\n",
       "      <td>3.946107</td>\n",
       "      <td>3.453044</td>\n",
       "      <td>6.400104</td>\n",
       "      <td>5.541883</td>\n",
       "      <td>5.770088</td>\n",
       "      <td>3.498807</td>\n",
       "      <td>7.286383</td>\n",
       "      <td>11.615904</td>\n",
       "      <td>...</td>\n",
       "      <td>3.533740</td>\n",
       "      <td>2.504072</td>\n",
       "      <td>2.331755</td>\n",
       "      <td>3.222780</td>\n",
       "      <td>2.633563</td>\n",
       "      <td>2.723859</td>\n",
       "      <td>2.724051</td>\n",
       "      <td>3.847020</td>\n",
       "      <td>3.566593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 12626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     100_g_at   1000_at   1001_at  1002_f_at  1003_s_at   1004_at   1005_at  \\\n",
       "0    6.927460  7.391657  3.812922   3.453385   6.070151  5.527153  5.812353   \n",
       "1    7.222432  7.329050  3.958028   3.407226   5.921265  5.376464  7.303408   \n",
       "2    6.776402  7.664007  3.783702   3.152019   5.452293  5.111794  7.207638   \n",
       "3    6.919134  7.469634  4.004581   3.341170   6.070925  5.296108  8.744059   \n",
       "4    7.113561  7.322408  4.242724   3.489324   6.141657  5.628390  6.825370   \n",
       "..        ...       ...       ...        ...        ...       ...       ...   \n",
       "97   6.819913  7.394450  3.617486   3.032897   5.414759  5.254673  8.785910   \n",
       "98   7.106082  7.465860  4.091138   3.402607   5.894284  5.206407  8.796626   \n",
       "99   6.907847  7.076667  3.998270   3.204424   5.912380  5.486270  6.133782   \n",
       "100  6.767213  7.170628  3.520863   3.413946   6.294376  5.528486  5.160356   \n",
       "101  7.066264  7.044501  3.946107   3.453044   6.400104  5.541883  5.770088   \n",
       "\n",
       "      1006_at  1007_s_at  1008_f_at  ...  AFFX-ThrX-5_at  AFFX-ThrX-M_at  \\\n",
       "0    3.167275   7.354981   9.419909  ...        3.770583        2.884436   \n",
       "1    3.108708   7.391872  10.539579  ...        3.190759        2.460119   \n",
       "2    3.077360   7.488371   6.833428  ...        3.325183        2.603014   \n",
       "3    3.117104   7.203028  10.400557  ...        3.625057        2.765521   \n",
       "4    3.794904   7.403024  10.240322  ...        3.698067        3.026876   \n",
       "..        ...        ...        ...  ...             ...             ...   \n",
       "97   3.671019   7.670415   9.357480  ...        3.922944        2.782813   \n",
       "98   3.154433   7.724618   7.752833  ...        3.491258        2.830696   \n",
       "99   3.874307   7.153055  10.432679  ...        3.512675        2.590794   \n",
       "100  3.367304   7.022459  11.443175  ...        3.654453        3.125644   \n",
       "101  3.498807   7.286383  11.615904  ...        3.533740        2.504072   \n",
       "\n",
       "     AFFX-TrpnX-3_at  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  AFFX-YEL002c/WBP1_at  \\\n",
       "0           2.730025         3.126168         2.870161              3.082210   \n",
       "1           2.696578         2.675271         2.940032              3.126269   \n",
       "2           2.469759         2.615746         2.510172              2.730814   \n",
       "3           2.681757         3.310741         3.197177              3.414182   \n",
       "4           2.691670         3.236030         3.003906              3.081497   \n",
       "..               ...              ...              ...                   ...   \n",
       "97          2.520608         2.885149         2.783539              2.554428   \n",
       "98          2.688828         3.054819         2.859602              2.883547   \n",
       "99          2.384583         2.890110         2.785744              2.755482   \n",
       "100         2.593132         3.655418         2.668599              2.901230   \n",
       "101         2.331755         3.222780         2.633563              2.723859   \n",
       "\n",
       "     AFFX-YEL018w/_at  AFFX-YEL021w/URA3_at  AFFX-YEL024w/RIP1_at  Y  \n",
       "0            2.747289              3.226588              3.480196  0  \n",
       "1            3.013745              3.517859              3.428752  1  \n",
       "2            2.613696              2.823436              3.049716  0  \n",
       "3            3.193867              3.353537              3.567482  0  \n",
       "4            2.963307              3.472050              3.598103  1  \n",
       "..                ...                   ...                   ... ..  \n",
       "97           2.733469              3.068130              2.953872  1  \n",
       "98           2.651160              3.306874              3.234820  0  \n",
       "99           2.731221              3.617421              3.255625  1  \n",
       "100          2.651258              3.174388              3.263090  1  \n",
       "101          2.724051              3.847020              3.566593  1  \n",
       "\n",
       "[102 rows x 12626 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prostate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1\n",
      " 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_prostate = prostate_data.values[:,:-1].astype(float)\n",
    "y_prostate = prostate_data.values[:, -1].astype(int)\n",
    "print(y_prostate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de los datos en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De cada conjunto de datos, dos tercios se emplearán para entrenar los modelos y el tercio restante se utilizará para testarlos. Además, realizaremos 20 particiones diferentes, de forma que el error de predicción será la media de los errores obtenidos con cada una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 done\n",
      "Partition 2 done\n",
      "Partition 3 done\n",
      "Partition 4 done\n",
      "Partition 5 done\n",
      "Partition 6 done\n",
      "Partition 7 done\n",
      "Partition 8 done\n",
      "Partition 9 done\n",
      "Partition 10 done\n",
      "Partition 11 done\n",
      "Partition 12 done\n",
      "Partition 13 done\n",
      "Partition 14 done\n",
      "Partition 15 done\n",
      "Partition 16 done\n",
      "Partition 17 done\n",
      "Partition 18 done\n",
      "Partition 19 done\n",
      "Partition 20 done\n"
     ]
    }
   ],
   "source": [
    "breast_complete = []\n",
    "for i in range(20):\n",
    "    bre_X_train, bre_X_test, bre_y_train, bre_y_test = train_test_split( \\\n",
    "        X_breast, y_breast, test_size= 1/3, random_state=i)\n",
    "    breast_complete.append([bre_X_train, bre_X_test, bre_y_train, bre_y_test])\n",
    "    print(f'Partition {i+1} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 done\n",
      "Partition 2 done\n",
      "Partition 3 done\n",
      "Partition 4 done\n",
      "Partition 5 done\n",
      "Partition 6 done\n",
      "Partition 7 done\n",
      "Partition 8 done\n",
      "Partition 9 done\n",
      "Partition 10 done\n",
      "Partition 11 done\n",
      "Partition 12 done\n",
      "Partition 13 done\n",
      "Partition 14 done\n",
      "Partition 15 done\n",
      "Partition 16 done\n",
      "Partition 17 done\n",
      "Partition 18 done\n",
      "Partition 19 done\n",
      "Partition 20 done\n"
     ]
    }
   ],
   "source": [
    "prostate_complete = []\n",
    "for i in range(20):\n",
    "    pro_X_train, pro_X_test, pro_y_train, pro_y_test = train_test_split(\\\n",
    "        X_prostate, y_prostate, test_size= 1/3, random_state=i)\n",
    "    prostate_complete.append([pro_X_train, pro_X_test, pro_y_train, pro_y_test])\n",
    "    print(f'Partition {i+1} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de los datos\n",
    "\n",
    "A continuación, transformamos los datos de manera que estos tengan una media de 0 y una desviación estándar de 1. Para ello, calculamos la media y la desviación estándar del conjunto de datos original utilizando solo los datos de entrenamiento, ya que los datos de prueba no deben ser utilizados en ninguna fase del entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breast dataset\n",
    "for i in breast_complete:\n",
    "    bre_X_train, bre_X_test, bre_y_train, bre_y_test=i\n",
    "    scaler = preprocessing.StandardScaler().fit(bre_X_train)\n",
    "    bre_X_train_scaled = scaler.transform(bre_X_train)\n",
    "    bre_X_test_scaled = scaler.transform(bre_X_test)\n",
    "    i.append(bre_X_train_scaled)\n",
    "    i.append(bre_X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prostate dataset\n",
    "for i in prostate_complete:\n",
    "    pro_X_train, pro_X_test, pro_y_train, pro_y_test=i\n",
    "    scaler = preprocessing.StandardScaler().fit(pro_X_train)\n",
    "    pro_X_train_scaled = scaler.transform(pro_X_train)\n",
    "    pro_X_test_scaled = scaler.transform(pro_X_test)\n",
    "    i.append(pro_X_train_scaled)\n",
    "    i.append(pro_X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos normalizado nuestros datos, procedemos a evaluar el primer clasificador: el clasificador de Naive Bayes.\n",
    "\n",
    "Este clasificador se basa en el teorema de Bayes, suponiendo que las características de los datos son independientes entre sí. Para cada instancia, el clasificador Naive Bayes calcula la probabilidad posterior para cada clase y asigna la clase con la probabilidad más alta.\n",
    "\n",
    "Las principales ventajas de este clasificador son su simplicidad y robustez frente al sobreaprendizaje, sin embargo, puede cometer errores cuando sí existe una relación de dependencia entre las características de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el clasificador, suponiendo que las características siguen una distribución normal\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de mama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.9\n",
      "Partition done. Prediction accuracy: 0.9368421052631579\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9368421052631579\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9210526315789473\n",
      "Partition done. Prediction accuracy: 0.9157894736842105\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9368421052631579\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9473684210526315\n",
      "Partition done. Prediction accuracy: 0.9105263157894737\n",
      "Partition done. Prediction accuracy: 0.9263157894736842\n",
      "The average prediction accuracy is 0.9347368421052632 (SD = 0.014259175983309532)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy=[]\n",
    "for i in breast_complete: # Para cada partición\n",
    "    bre_X_train, bre_X_test, bre_y_train, bre_y_test, bre_X_train_scaled, bre_X_test_scaled=i\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    nb.fit(bre_X_train_scaled, bre_y_train)\n",
    "    # Predecimos la clase de los datos de test\n",
    "    bre_y_pred = nb.predict(bre_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(bre_y_test, bre_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de próstata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.6764705882352942\n",
      "Partition done. Prediction accuracy: 0.8235294117647058\n",
      "Partition done. Prediction accuracy: 0.4411764705882353\n",
      "Partition done. Prediction accuracy: 0.7352941176470589\n",
      "Partition done. Prediction accuracy: 0.7058823529411765\n",
      "Partition done. Prediction accuracy: 0.7647058823529411\n",
      "Partition done. Prediction accuracy: 0.5882352941176471\n",
      "Partition done. Prediction accuracy: 0.7058823529411765\n",
      "Partition done. Prediction accuracy: 0.7647058823529411\n",
      "Partition done. Prediction accuracy: 0.5\n",
      "Partition done. Prediction accuracy: 0.5588235294117647\n",
      "Partition done. Prediction accuracy: 0.4411764705882353\n",
      "Partition done. Prediction accuracy: 0.6470588235294118\n",
      "Partition done. Prediction accuracy: 0.7352941176470589\n",
      "Partition done. Prediction accuracy: 0.5588235294117647\n",
      "Partition done. Prediction accuracy: 0.5\n",
      "Partition done. Prediction accuracy: 0.5294117647058824\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.6176470588235294\n",
      "Partition done. Prediction accuracy: 0.7647058823529411\n",
      "The average prediction accuracy is 0.6455882352941178 (SD = 0.12250097097277782)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy=[]\n",
    "for i in prostate_complete: # Para cada partición\n",
    "    pro_X_train, pro_X_test, pro_y_train, pro_y_test, pro_X_train_scaled, pro_X_test_scaled=i\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    nb.fit(pro_X_train_scaled, pro_y_train)\n",
    "    # Predecimos la clase de los datos de test\n",
    "    pro_y_pred = nb.predict(pro_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(pro_y_test, pro_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, el clasificador Naive Bayes funciona mejor con el conjunto de datos de cáncer de mama que con el de cáncer de próstata, probablemente debido a lo que se conoce como maldición de la dimensionalidad.\n",
    "\n",
    "Pasamos ahora a evaluar el análisis discriminante:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen principalmente dos tipos de análisis discriminante: LDA (Linear Discriminant Analysis) y QDA (Quadratic Discriminant Analysis). \n",
    "\n",
    "LDA intenta encontrar una frontera lineal entre las diferentes clases basándose en la suposición de que todas las clases tienen la misma varianza-covarianza. QDA, sin embargo, deja que cada clase tenga su propia matriz de varianza-covarianza, lo que le permite modelar fronteras no lineales entre las clases.\n",
    "\n",
    "LDA es un clasificador más sencillo, menos flexible pero más robusto frente al sobreaprendizaje.\n",
    "\n",
    "QDA es un clasificador más complejo, más flexible pero también más propenso al sobreaprendizaje, por lo que puede no ser adecuado cuando el conjunto de datos es pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el clasificador\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de mama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.9736842105263158\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.9789473684210527\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.9473684210526315\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9578947368421052\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9578947368421052\n",
      "Partition done. Prediction accuracy: 0.9578947368421052\n",
      "Partition done. Prediction accuracy: 0.968421052631579\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "The average prediction accuracy is 0.9584210526315792 (SD = 0.009832390364352333)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy=[]\n",
    "for i in breast_complete: # Para cada partición\n",
    "    bre_X_train, bre_X_test, bre_y_train, bre_y_test, bre_X_train_scaled, bre_X_test_scaled=i\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    lda.fit(bre_X_train_scaled, bre_y_train)\n",
    "    # Predecimos la clase de los datos de test\n",
    "    bre_y_pred = lda.predict(bre_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(bre_y_test, bre_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de próstata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.7941176470588235\n",
      "Partition done. Prediction accuracy: 0.8823529411764706\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8235294117647058\n",
      "Partition done. Prediction accuracy: 0.8235294117647058\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.7941176470588235\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8823529411764706\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.7647058823529411\n",
      "Partition done. Prediction accuracy: 0.7941176470588235\n",
      "Partition done. Prediction accuracy: 0.8235294117647058\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8235294117647058\n",
      "The average prediction accuracy is 0.8455882352941178 (SD = 0.04040774018833555)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy=[]\n",
    "for i in prostate_complete: # Para cada partición\n",
    "    pro_X_train, pro_X_test, pro_y_train, pro_y_test, pro_X_train_scaled, pro_X_test_scaled=i\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    lda.fit(pro_X_train_scaled, pro_y_train)\n",
    "    # Predecimos la clase de los datos de test\n",
    "    pro_y_pred = lda.predict(pro_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(pro_y_test, pro_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que el clasificador de Naive Bayes, este clasificador funciona mejor con el conjunto de datos de cáncer de mama que con el de cáncer de próstata, también por la maldición de la dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de entrenar el clasificador, tenemos que elegir un valor adecuado para el parámetro de regularización. Para ello, empleamos la técnica de grid-search, guiada por validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([ ('qda', QuadraticDiscriminantAnalysis()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param_values = np.linspace(0, 1, 10).tolist()\n",
    "param_grid = { 'qda__reg_param': reg_param_values }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de mama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.9578947368421052\n",
      "Partition done. Prediction accuracy: 0.9789473684210527\n",
      "Partition done. Prediction accuracy: 0.968421052631579\n",
      "Partition done. Prediction accuracy: 0.9789473684210527\n",
      "Partition done. Prediction accuracy: 0.968421052631579\n",
      "Partition done. Prediction accuracy: 0.968421052631579\n",
      "Partition done. Prediction accuracy: 0.968421052631579\n",
      "Partition done. Prediction accuracy: 0.9736842105263158\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.968421052631579\n",
      "Partition done. Prediction accuracy: 0.9842105263157894\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9473684210526315\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "Partition done. Prediction accuracy: 0.9736842105263158\n",
      "Partition done. Prediction accuracy: 0.9736842105263158\n",
      "Partition done. Prediction accuracy: 0.9736842105263158\n",
      "Partition done. Prediction accuracy: 0.9631578947368421\n",
      "Partition done. Prediction accuracy: 0.968421052631579\n",
      "Partition done. Prediction accuracy: 0.9526315789473684\n",
      "The average prediction accuracy is 0.966842105263158 (SD = 0.009719044901378645)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy = []\n",
    "\n",
    "for i in breast_complete: # Para cada partición\n",
    "    bre_X_train, bre_X_test, bre_y_train, bre_y_test, bre_X_train_scaled, bre_X_test_scaled = i\n",
    "    \n",
    "    # Buscamos el mejor valor del parámetro de regularización utilizando solo los datos de entrenamiento\n",
    "    skfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=0)\n",
    "    gridcv = GridSearchCV(pipeline, cv=skfold, n_jobs=1, param_grid=param_grid, \\\n",
    "        scoring = make_scorer(accuracy_score))\n",
    "    result = gridcv.fit(bre_X_train_scaled, bre_y_train)\n",
    "    accuracies = gridcv.cv_results_['mean_test_score']\n",
    "    best_reg_param_value = reg_param_values[max(enumerate(accuracies), key=lambda x: x[1])[0]]\n",
    "    \n",
    "    # Creamos el clasificador con el mejor valor del parámetro de regularización\n",
    "    qda = QuadraticDiscriminantAnalysis(reg_param = best_reg_param_value)\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    qda.fit(bre_X_train_scaled, bre_y_train)  \n",
    "    # Predecimos la clase de los datos de test\n",
    "    bre_y_pred = qda.predict(bre_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(bre_y_test, bre_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de próstata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.5588235294117647\n",
      "Partition done. Prediction accuracy: 0.6764705882352942\n",
      "Partition done. Prediction accuracy: 0.4411764705882353\n",
      "Partition done. Prediction accuracy: 0.6764705882352942\n",
      "Partition done. Prediction accuracy: 0.6470588235294118\n",
      "Partition done. Prediction accuracy: 0.7058823529411765\n",
      "Partition done. Prediction accuracy: 0.7647058823529411\n",
      "Partition done. Prediction accuracy: 0.7941176470588235\n",
      "Partition done. Prediction accuracy: 0.7941176470588235\n",
      "Partition done. Prediction accuracy: 0.5\n",
      "Partition done. Prediction accuracy: 0.5882352941176471\n",
      "Partition done. Prediction accuracy: 0.7058823529411765\n",
      "Partition done. Prediction accuracy: 0.7352941176470589\n",
      "Partition done. Prediction accuracy: 0.7058823529411765\n",
      "Partition done. Prediction accuracy: 0.5\n",
      "Partition done. Prediction accuracy: 0.5588235294117647\n",
      "Partition done. Prediction accuracy: 0.5\n",
      "Partition done. Prediction accuracy: 0.47058823529411764\n",
      "Partition done. Prediction accuracy: 0.5294117647058824\n",
      "Partition done. Prediction accuracy: 0.7058823529411765\n",
      "The average prediction accuracy is 0.6279411764705882 (SD = 0.11054872573325156)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy = []\n",
    "\n",
    "for i in prostate_complete: # Para cada partición\n",
    "    pro_X_train, pro_X_test, pro_y_train, pro_y_test, pro_X_train_scaled, pro_X_test_scaled = i\n",
    "    \n",
    "    # Buscamos el mejor valor del parámetro de regularización utilizando solo los datos de entrenamiento\n",
    "    skfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=0)\n",
    "    gridcv = GridSearchCV(pipeline, cv=skfold, n_jobs=1, param_grid=param_grid, \\\n",
    "        scoring = make_scorer(accuracy_score))\n",
    "    result = gridcv.fit(pro_X_train_scaled, pro_y_train)\n",
    "    accuracies = gridcv.cv_results_['mean_test_score']\n",
    "    best_reg_param_value = reg_param_values[max(enumerate(accuracies), key=lambda x: x[1])[0]]\n",
    "\n",
    "    # Creamos el clasificador con el mejor valor del parámetro de regularización\n",
    "    qda = QuadraticDiscriminantAnalysis(reg_param = best_reg_param_value)\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    qda.fit(pro_X_train_scaled, pro_y_train)\n",
    "    # Predecimos la clase de los datos de test\n",
    "    pro_y_pred = qda.predict(pro_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(pro_y_test, pro_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como suponíamos, QDA funciona bastante peor con el conjunto de datos de cáncer de próstata, ya que en este conjunto el número de datos es mucho menor que el número de dimensiones y QDA es un clasificador propenso al sobreaprendizaje. \n",
    "\n",
    "Sin embargo, para el conjunto de datos de cáncer de mama, QDA funciona algo mejor que LDA, ya que es un clasificador más flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Shrunken Centroids Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que para QDA, antes de entrenar el clasificador tenemos que elegir valores adecuados para los hiperparámetros. Para ello, de nuevo empleamos la técnica de grid-search, guiada por validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([ ('nsc', NearestCentroid()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinkage_param_values = np.linspace(1e-6, 8, 20).tolist()\n",
    "param_grid_nsc = {'nsc__shrink_threshold': shrinkage_param_values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de mama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.9157894736842105\n",
      "Partition done. Prediction accuracy: 0.9263157894736842\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9578947368421052\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9263157894736842\n",
      "Partition done. Prediction accuracy: 0.9473684210526315\n",
      "Partition done. Prediction accuracy: 0.9578947368421052\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9368421052631579\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9421052631578948\n",
      "Partition done. Prediction accuracy: 0.9315789473684211\n",
      "Partition done. Prediction accuracy: 0.9368421052631579\n",
      "Partition done. Prediction accuracy: 0.9263157894736842\n",
      "The average prediction accuracy is 0.9376315789473685 (SD = 0.010161439250688332)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy = []\n",
    "\n",
    "for i in breast_complete: # Para cada partición\n",
    "    bre_X_train, bre_X_test, bre_y_train, bre_y_test, bre_X_train_scaled, bre_X_test_scaled = i\n",
    "\n",
    "    # Buscamos el mejor valor del hiperparámetro utilizando solo los datos de entrenamiento y creamos el clasificador\n",
    "    gridcv_nsc = GridSearchCV(pipeline, cv=skfold, n_jobs=1, param_grid=param_grid_nsc, \\\n",
    "        scoring=make_scorer(accuracy_score))\n",
    "    result_nsc = gridcv_nsc.fit(bre_X_train_scaled, bre_y_train)\n",
    "    accuracies = gridcv_nsc.cv_results_['mean_test_score']\n",
    "    nsc = NearestCentroid(shrink_threshold = shrinkage_param_values[np.argmax(accuracies)])\n",
    "\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    nsc.fit(bre_X_train_scaled, bre_y_train)\n",
    "    # Predecimos la clase de los datos de test\n",
    "    bre_y_pred = nsc.predict(bre_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(bre_y_test, bre_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el conjunto de datos de cáncer de próstata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition done. Prediction accuracy: 0.9411764705882353\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.9411764705882353\n",
      "Partition done. Prediction accuracy: 0.9411764705882353\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8823529411764706\n",
      "Partition done. Prediction accuracy: 0.8235294117647058\n",
      "Partition done. Prediction accuracy: 0.7941176470588235\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.8823529411764706\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.8235294117647058\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "Partition done. Prediction accuracy: 0.9117647058823529\n",
      "Partition done. Prediction accuracy: 0.9705882352941176\n",
      "Partition done. Prediction accuracy: 0.8529411764705882\n",
      "The average prediction accuracy is 0.8867647058823529 (SD = 0.04582481309083202)\n"
     ]
    }
   ],
   "source": [
    "pred_accuracy = []\n",
    "\n",
    "for i in prostate_complete: # Para cada partición\n",
    "    pro_X_train, pro_X_test, pro_y_train, pro_y_test, pro_X_train_scaled, pro_X_test_scaled = i\n",
    "\n",
    "    # Buscamos el mejor valor del hiperparámetro utilizando solo los datos de entrenamiento y creamos el clasificador\n",
    "    gridcv_nsc = GridSearchCV(pipeline, cv=skfold, n_jobs=1, param_grid=param_grid_nsc, \\\n",
    "        scoring=make_scorer(accuracy_score))\n",
    "    result_nsc = gridcv_nsc.fit(pro_X_train_scaled, pro_y_train)\n",
    "    accuracies = gridcv_nsc.cv_results_['mean_test_score']\n",
    "    nsc = NearestCentroid(shrink_threshold = shrinkage_param_values[ np.argmax(accuracies)])\n",
    "\n",
    "    # Entrenamos el clasificador con los datos de entrenamiento normalizados\n",
    "    nsc.fit(pro_X_train_scaled, pro_y_train)\n",
    "    # Predecimos la clase de los datos de test\n",
    "    pro_y_pred = nsc.predict(pro_X_test_scaled)\n",
    "    # Calculamos la matriz de confusión\n",
    "    conf = confusion_matrix(pro_y_test, pro_y_pred)\n",
    "    TN = conf[0][0]\n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    # Calculamos la precisión de la predicción y la añadimos a la lista pred_accuracy\n",
    "    pred_accuracy.append(((TP + TN) / (TN + TP + FP + FN)))\n",
    "    print(f'Partition done. Prediction accuracy: {((TP + TN) / (TN + TP + FP + FN))}')\n",
    "\n",
    "pred_accuracy = np.array(pred_accuracy)\n",
    "print(F\"The average prediction accuracy is {pred_accuracy.mean()} (SD = {pred_accuracy.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, el clasificador funciona mejor con el conjunto de datos de cáncer de mama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREGUNTAS\n",
    "- What method performs best on each dataset in terms of prediction error?\n",
    "\n",
    "El error de predicción puede calcularse como 1 - prediction accuracy, por lo que el clasificador que funciona mejor es aquel que tiene mayor prediction accuracy (y por tanto menor error de predicción). Teniendo esto en cuenta, el clasificador que funciona mejor con el conjunto de datos de cáncer de mama es QDA y el que funciona mejor con el conjunto de datos de cáncer de próstata es NSC.\n",
    "\n",
    "- What method is more flexible according to what we have seen in the lectures? Is this compatible with the obtained results?\n",
    "\n",
    "El clasificador más flexible (y que por tanto debería ajustarse mejor a los datos) es NSC que, como hemos visto, funciona bien tanto con el conjunto de datos de cáncer de mama como con el conjunto de datos de cáncer de próstata.\n",
    "\n",
    "QDA también es flexible, aunque no funciona también con el conjunto de datos de cáncer de próstata debido al sobreaprendizaje.\n",
    "\n",
    "- What method is more robust to over-fitting according to what we have seen in the lectures? Is this compatible with the obtained results?\n",
    "\n",
    "Los clasificadores más robustos frente al sobreaprendizaje son los más sencillos, como Naive Bayes o LDA. Después de NSC, LDA es el clasificador que mejor funciona con el conjunto de datos de cáncer de próstata, lo que concuerda con lo que cabría esperar.\n",
    "\n",
    "- Discuss and compare, regarding the bias and the variance, the procedure used to estimate the generalization error of a classifier trained on the available data with 10-fold-cross validation and leave-one-out cross-validation. Which one has a bigger bias and a bigger variance? Explain your response. You need not carry out additional experiments, only explain what you should expect to obtain according to what you have seen in the lectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
