{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a127746",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Enrique Juliá Arévalo, Sara Verde Camacho, Leo Pérez Peña"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb36921",
   "metadata": {},
   "source": [
    "Se comienza cargando los paquetes necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4b84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "import sys\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432adff",
   "metadata": {},
   "source": [
    "**In this practical, you will become familiarized with some basic feature selection methods implemented in scikit-learn. Consider the prostate dataset that is attached to this practical. You are asked to:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a052b",
   "metadata": {},
   "source": [
    "Se carga el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ba0541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100_g_at</th>\n",
       "      <th>1000_at</th>\n",
       "      <th>1001_at</th>\n",
       "      <th>1002_f_at</th>\n",
       "      <th>1003_s_at</th>\n",
       "      <th>1004_at</th>\n",
       "      <th>1005_at</th>\n",
       "      <th>1006_at</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1008_f_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "      <th>AFFX-YEL002c/WBP1_at</th>\n",
       "      <th>AFFX-YEL018w/_at</th>\n",
       "      <th>AFFX-YEL021w/URA3_at</th>\n",
       "      <th>AFFX-YEL024w/RIP1_at</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.927460</td>\n",
       "      <td>7.391657</td>\n",
       "      <td>3.812922</td>\n",
       "      <td>3.453385</td>\n",
       "      <td>6.070151</td>\n",
       "      <td>5.527153</td>\n",
       "      <td>5.812353</td>\n",
       "      <td>3.167275</td>\n",
       "      <td>7.354981</td>\n",
       "      <td>9.419909</td>\n",
       "      <td>...</td>\n",
       "      <td>3.770583</td>\n",
       "      <td>2.884436</td>\n",
       "      <td>2.730025</td>\n",
       "      <td>3.126168</td>\n",
       "      <td>2.870161</td>\n",
       "      <td>3.082210</td>\n",
       "      <td>2.747289</td>\n",
       "      <td>3.226588</td>\n",
       "      <td>3.480196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.222432</td>\n",
       "      <td>7.329050</td>\n",
       "      <td>3.958028</td>\n",
       "      <td>3.407226</td>\n",
       "      <td>5.921265</td>\n",
       "      <td>5.376464</td>\n",
       "      <td>7.303408</td>\n",
       "      <td>3.108708</td>\n",
       "      <td>7.391872</td>\n",
       "      <td>10.539579</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190759</td>\n",
       "      <td>2.460119</td>\n",
       "      <td>2.696578</td>\n",
       "      <td>2.675271</td>\n",
       "      <td>2.940032</td>\n",
       "      <td>3.126269</td>\n",
       "      <td>3.013745</td>\n",
       "      <td>3.517859</td>\n",
       "      <td>3.428752</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.776402</td>\n",
       "      <td>7.664007</td>\n",
       "      <td>3.783702</td>\n",
       "      <td>3.152019</td>\n",
       "      <td>5.452293</td>\n",
       "      <td>5.111794</td>\n",
       "      <td>7.207638</td>\n",
       "      <td>3.077360</td>\n",
       "      <td>7.488371</td>\n",
       "      <td>6.833428</td>\n",
       "      <td>...</td>\n",
       "      <td>3.325183</td>\n",
       "      <td>2.603014</td>\n",
       "      <td>2.469759</td>\n",
       "      <td>2.615746</td>\n",
       "      <td>2.510172</td>\n",
       "      <td>2.730814</td>\n",
       "      <td>2.613696</td>\n",
       "      <td>2.823436</td>\n",
       "      <td>3.049716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.919134</td>\n",
       "      <td>7.469634</td>\n",
       "      <td>4.004581</td>\n",
       "      <td>3.341170</td>\n",
       "      <td>6.070925</td>\n",
       "      <td>5.296108</td>\n",
       "      <td>8.744059</td>\n",
       "      <td>3.117104</td>\n",
       "      <td>7.203028</td>\n",
       "      <td>10.400557</td>\n",
       "      <td>...</td>\n",
       "      <td>3.625057</td>\n",
       "      <td>2.765521</td>\n",
       "      <td>2.681757</td>\n",
       "      <td>3.310741</td>\n",
       "      <td>3.197177</td>\n",
       "      <td>3.414182</td>\n",
       "      <td>3.193867</td>\n",
       "      <td>3.353537</td>\n",
       "      <td>3.567482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.113561</td>\n",
       "      <td>7.322408</td>\n",
       "      <td>4.242724</td>\n",
       "      <td>3.489324</td>\n",
       "      <td>6.141657</td>\n",
       "      <td>5.628390</td>\n",
       "      <td>6.825370</td>\n",
       "      <td>3.794904</td>\n",
       "      <td>7.403024</td>\n",
       "      <td>10.240322</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698067</td>\n",
       "      <td>3.026876</td>\n",
       "      <td>2.691670</td>\n",
       "      <td>3.236030</td>\n",
       "      <td>3.003906</td>\n",
       "      <td>3.081497</td>\n",
       "      <td>2.963307</td>\n",
       "      <td>3.472050</td>\n",
       "      <td>3.598103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   100_g_at   1000_at   1001_at  1002_f_at  1003_s_at   1004_at   1005_at  \\\n",
       "0  6.927460  7.391657  3.812922   3.453385   6.070151  5.527153  5.812353   \n",
       "1  7.222432  7.329050  3.958028   3.407226   5.921265  5.376464  7.303408   \n",
       "2  6.776402  7.664007  3.783702   3.152019   5.452293  5.111794  7.207638   \n",
       "3  6.919134  7.469634  4.004581   3.341170   6.070925  5.296108  8.744059   \n",
       "4  7.113561  7.322408  4.242724   3.489324   6.141657  5.628390  6.825370   \n",
       "\n",
       "    1006_at  1007_s_at  1008_f_at  ...  AFFX-ThrX-5_at  AFFX-ThrX-M_at  \\\n",
       "0  3.167275   7.354981   9.419909  ...        3.770583        2.884436   \n",
       "1  3.108708   7.391872  10.539579  ...        3.190759        2.460119   \n",
       "2  3.077360   7.488371   6.833428  ...        3.325183        2.603014   \n",
       "3  3.117104   7.203028  10.400557  ...        3.625057        2.765521   \n",
       "4  3.794904   7.403024  10.240322  ...        3.698067        3.026876   \n",
       "\n",
       "   AFFX-TrpnX-3_at  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  AFFX-YEL002c/WBP1_at  \\\n",
       "0         2.730025         3.126168         2.870161              3.082210   \n",
       "1         2.696578         2.675271         2.940032              3.126269   \n",
       "2         2.469759         2.615746         2.510172              2.730814   \n",
       "3         2.681757         3.310741         3.197177              3.414182   \n",
       "4         2.691670         3.236030         3.003906              3.081497   \n",
       "\n",
       "   AFFX-YEL018w/_at  AFFX-YEL021w/URA3_at  AFFX-YEL024w/RIP1_at  Y  \n",
       "0          2.747289              3.226588              3.480196  0  \n",
       "1          3.013745              3.517859              3.428752  1  \n",
       "2          2.613696              2.823436              3.049716  0  \n",
       "3          3.193867              3.353537              3.567482  0  \n",
       "4          2.963307              3.472050              3.598103  1  \n",
       "\n",
       "[5 rows x 12626 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('prostate.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723b3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data.iloc[:, :-1]).astype(float)\n",
    "y = np.array(data.iloc[:, -1]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840747d8",
   "metadata": {},
   "source": [
    "1. Estimate the performance of the nearest neighbor classifier on this dataset using 10-times 10-fold cross validation when all the features are used for prediction. The number of neighbors should be chosen using an inner cross-validation procedure. You can use 5-fold cross validation for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce4093",
   "metadata": {},
   "source": [
    "El clasificador de nearest neighbor consiste en asignar una etiqueta en base a la etiqueta que tengan la mayor parte de los *k* vecinos más cercanos. Para este ejercicio será necesario, por un lado, obtener el número óptimo de vecinos. Esto se determinará haciendo una validación cruzada, con una partición en 5 de los datos. Como se han partido en 5, se repetirá el siguiente proceso 5 veces: se seleccionará uno de los grupos para test, y con los otros cuatro se ajustará el modelo para cada uno de los valores posibles de *k*, y el que dé menos error al etiquetar el de test será el que se seleccionará. Esta optimización \"interna\" se llevará a cabo con los datos de entrenamiento del ajuste \"externo\", de forma que, una vez se hayan predecido el número óptimo de vecinos, el ajuste externo de los datos de entrenamiento se hará con este número de vecinos. Este ajuste externo se repetirá 10 veces, y se utilizará la totalidad de características de los datos para ello. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e128043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7936363636363637 ± 0.14013275877768422\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'knn__n_neighbors': np.arange(1, 21)}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# validación cruzada externa: 10-fold\n",
    "out_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# validación cruzada interna: 5-fold, para determinar el mejor numero de vecinos\n",
    "in_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# validación cruzada interna: 5-fold, para determinar el mejor número de vecinos\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, cv=in_cv)\n",
    "\n",
    "# entrenar el clasificador con la validación cruzada interna\n",
    "nested_scores = cross_val_score(clf, X, y, cv=out_cv)\n",
    "\n",
    "print(f\"Accuracy: {nested_scores.mean()} ± {nested_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48992bf",
   "metadata": {},
   "source": [
    "Versión más larga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8668582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, K: 9, accuracy = 0.7272727272727273\n",
      "Iteration: 2, K: 9, accuracy = 1.0\n",
      "Iteration: 3, K: 9, accuracy = 0.7\n",
      "Iteration: 4, K: 5, accuracy = 0.9\n",
      "Iteration: 5, K: 7, accuracy = 0.8\n",
      "Iteration: 6, K: 3, accuracy = 0.8\n",
      "Iteration: 7, K: 7, accuracy = 1.0\n",
      "Iteration: 8, K: 3, accuracy = 0.8\n",
      "Iteration: 9, K: 8, accuracy = 0.8\n",
      "Iteration: 10, K: 4, accuracy = 0.6\n"
     ]
    }
   ],
   "source": [
    "k_values = np.arange(3, 10) # menor rango porque tarda mucho\n",
    "out_scores = []\n",
    "score_per_k = {}\n",
    "\n",
    "for out_i, (train_val_index, test_index) in enumerate(out_cv.split(X, y)):\n",
    "    X_train_val, X_test = X[train_val_index], X[test_index]\n",
    "    y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train_val)\n",
    "    X_train_scaled = scaler.transform(X_train_val)\n",
    "    X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "    mean_in_score = []\n",
    "\n",
    "    for k in k_values:\n",
    "        in_score = []\n",
    "\n",
    "        for train_index, val_index in in_cv.split(X_train_val, y_train_val):\n",
    "            X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "            y_train, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "\n",
    "            scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "            in_X_train_scaled = scaler.transform(X_train)\n",
    "            in_X_test_scaled = scaler.transform(X_val)\n",
    "            \n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            knn.fit(in_X_train_scaled, y_train)\n",
    "            acc = accuracy_score(y_val, knn.predict(in_X_test_scaled))\n",
    "            in_score.append(acc)\n",
    "        \n",
    "        mean_in_score.append(np.mean(in_score))\n",
    "    \n",
    "    final_k = k_values[np.argmax(mean_in_score)]\n",
    "    final_knn =KNeighborsClassifier(n_neighbors= final_k)\n",
    "    final_knn.fit(X_train_scaled, y_train_val)\n",
    "    acc = accuracy_score(y_test, final_knn.predict(X_test_scaled))\n",
    "    out_scores.append(acc)\n",
    "    print(f\"Iteration: {out_i + 1}, K: {final_k}, accuracy = {acc}\")\n",
    "    \n",
    "    if final_k not in score_per_k:\n",
    "        score_per_k[final_k] = [acc]\n",
    "    else:\n",
    "        score_per_k[final_k].append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb922b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8127272727272727\n"
     ]
    }
   ],
   "source": [
    "out_scores = np.array(out_scores)\n",
    "print(f\"Mean accuracy: {out_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "141ae52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for 9 neighbors: 0.8090909090909091\n",
      "Average accuracy for 5 neighbors: 0.9\n",
      "Average accuracy for 7 neighbors: 0.9\n",
      "Average accuracy for 3 neighbors: 0.8\n",
      "Average accuracy for 8 neighbors: 0.8\n",
      "Average accuracy for 4 neighbors: 0.6\n"
     ]
    }
   ],
   "source": [
    "for k in score_per_k:\n",
    "    print(f\"Average accuracy for {k} neighbors: {np.array(score_per_k[k]).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b381ca",
   "metadata": {},
   "source": [
    "2. Estimate the performance of the nearest neighbor classifier on the same dataset when using a feature selection technique based on the F-score (ANOVA) that picks up the 10 most relevant features. Use the same cross-validation methods as in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43830510",
   "metadata": {},
   "source": [
    "Se repetirá lo mismo que antes, pero en este caso, se comenzará seleccionando el número de características empleando solo las 10 más relevantes utilizando ANOVA para determinar cuáles son. ANOVA se basa en el cálculo de la varianza inter e intragrupo y las compara. Esto permite que se seleccionen las características que maximizan la varianza intergrupo, utilizando únicamente estas para el ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe2449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9209090909090909 ± 0.09783186791718376\n"
     ]
    }
   ],
   "source": [
    "# se escalan los datos, se seleccionan las características y \n",
    "# se entrena el clasificador\n",
    "pipe = Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# valores posibles para el número de vecinos\n",
    "param_grid = {'knn__n_neighbors': np.arange(1, 21)}\n",
    "\n",
    "# validación cruzada externa: 10-fold \n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# validación cruzada interna: 5-fold -> para determinar el número de vecinos\n",
    "# una vez que se han seleccionado las 10 características más relevantes\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# se entrena el clasificador con la validación cruzada interna\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, cv=inner_cv)\n",
    "\n",
    "# se entrena el clasificador con la validación cruzada externa, \n",
    "# con el mejor número de vecinos (determinado por la validación cruzada interna)\n",
    "nested_scores = cross_val_score(clf, X, y, cv=outer_cv)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {nested_scores.mean()} ± {nested_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd6c91",
   "metadata": {},
   "source": [
    "3. Repeat the previous experiment but when a random forest is used to pick up the 10 most relevant features. Use an initial filtering method based on the F-score to keep only the 20% most promising features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955c6cd",
   "metadata": {},
   "source": [
    "Al igual que antes, se emplearán solo las 10 características más relevantes. En este caso, se determinarán a través de random forest. La utilización de random forest en la selección de características consiste en seleccionar aquellas que permiten que las hojas de los árboles sean lo más puras que sea posible. Para ello se generan árboles y se van añadiendo características. Aquellas que aumenten en mayor medida la pureza de las hojas serán las que se seleccionen. Se partirá del 20% de características, que se seleccionarán a través de un ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validación cruzada anidada\n",
    "    # se selecciona el 20% de las características con F-test\n",
    "filtering = SelectKBest(score_func=f_classif, k= int(np.round(X.shape[ 1 ] * 0.2)))\n",
    "    # se selecciona 10 de esas características con Random Forest\n",
    "\n",
    "# se da a las características su importancia determinada por el Random Forest\n",
    "rf_selection =  SelectFromModel(RandomForestClassifier(n_estimators = 2000, \\\n",
    "    random_state = 0), threshold = 0.0) \n",
    "\n",
    "# se define el fold externo \n",
    "n_repeats = 1\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats = n_repeats, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbb204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    }
   ],
   "source": [
    "accuracy = np.zeros(10 * n_repeats)\n",
    "np.random.seed(0)\n",
    "split = 0\n",
    "\n",
    "for train_index, test_index in rkf.split(X, y):\n",
    "\n",
    "    print(f\"Iteration #:{split + 1}\")\n",
    "    \n",
    "    # se separan los datos en entrenamiento y test\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # se escalan los datos en base a los datos de entrenamiento\n",
    "    scaler.fit(X_train, y_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # se seleccionan las características con ANOVA\n",
    "    filtering.fit(X_train, y_train)\n",
    "    X_train_rf = filtering.transform(X_train)\n",
    "    X_test_rf = filtering.transform(X_test)\n",
    "    \n",
    "    # se utiliza la función para ajustar los datos a Random Forest\n",
    "    rf_selection.fit(X_train_rf, y_train)\n",
    "    # se cambia el umbral para seleccionar las 10 características más relevantes\n",
    "    rf_selection.threshold = -1.0 * np.sort(-1.0 * rf_selection.estimator_.feature_importances_)[ 9 ]\n",
    "    # se seleccionan las características con Random Forest\n",
    "    X_train_rf = rf_selection.transform(X_train_rf)\n",
    "    X_test_rf = rf_selection.transform(X_test_rf)\n",
    "    \n",
    "    # se entrena el clasificador con el número de vecinos determinado por la validación cruzada interna\n",
    "    knn.fit(X_train_rf, y_train)\n",
    "\n",
    "    # se determina la precisión del clasificador\n",
    "    accuracy[ split ] =  np.mean(knn.predict(X_test_rf) == y_test)\n",
    "    \n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e2ac68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy KNN:0.920000\n",
      "\tStd Mean Error KNN:0.027568\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy KNN:%f\" % np.mean(accuracy))\n",
    "print(\"\\tStd Mean Error KNN:%f\" % (np.std(accuracy) / np.sqrt(len(accuracy))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3031f65",
   "metadata": {},
   "source": [
    "4. What feature selection method performs best? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6342b",
   "metadata": {},
   "source": [
    "**Now we will address the problem of analyzing the trade-off between interpretability and prediction accuracy. For this, you are asked to:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72f722",
   "metadata": {},
   "source": [
    "1. Estimate the performance of the nearest neighbor classifier with K=3 as a function of the features used for prediction. Use a 10-times 10-fold cross-validation method and plot the results obtained. That is prediction error vs. the number of features used for prediction. Use the F-score for feature selection. Report results from 1 feature to 200 features. Not all features need to be explored. Use a higher resolution when you are closer to 1 feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a45da",
   "metadata": {},
   "source": [
    "2. Repeat that process when the feature selection is done externally to the cross-validation loop using all the available data. Include these results in the previous plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7f100",
   "metadata": {},
   "source": [
    "3. Are the two estimates obtained similar? What are their differences? If they are different try to explain why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee82b26",
   "metadata": {},
   "source": [
    "4. By taking a look at these results, what is the optimal number of features to use in this dataset in terms of interpretability vs. prediction error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92700c17",
   "metadata": {},
   "source": [
    "5. Given the results obtained in this part of the practical, you are asked to indicate which particular features should be used for prediction on this dataset. Include a list with them. Take a look at the documentation of SelectKBest from scikit-learn to understand how to do this. Use all available data to provide such a list of features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
